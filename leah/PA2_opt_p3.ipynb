{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 DSC 102 SP25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we will conduct data engineering for the Amazon dataset. It is divided into 2 parts. The extracted features in Part 1 will be used for the Part 2 of assignment, where you train a model (or models) to predict user ratings for a product.\n",
    "\n",
    "We will be using Apache Spark for this assignment. The default Spark API will be DataFrame, as it is now the recommended choice over the RDD API. That being said, please feel free to switch back to the RDD API if you see it as a better fit for the task. We provide you an option to request RDD format to start with. Also you can switch between DataFrame and RDD in your solution. \n",
    "\n",
    "Another newer API is Koalas, which is also avaliable. However, it has constraints and is not applicable to most tasks. Refer to the PA statement for detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T18:47:30.022030Z",
     "start_time": "2020-02-03T18:47:30.019499Z"
    }
   },
   "outputs": [],
   "source": [
    "PID = 'A17680599' # your pid, for instance: 'a43223333'\n",
    "INPUT_FORMAT = 'dataframe' # choose a format of your input data, valid options: 'dataframe', 'rdd', 'koalas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T18:50:16.780690Z",
     "start_time": "2020-02-03T18:47:30.263681Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/29 02:03:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/opt/bitnami/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Boiler plates, do NOT modify\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "from utilities import SEED\n",
    "from utilities import PA2Test\n",
    "from utilities import PA2Data\n",
    "from utilities import data_cat\n",
    "from pa2_main import PA2Executor\n",
    "import time\n",
    "if INPUT_FORMAT == 'dataframe':\n",
    "    import pyspark.ml as M\n",
    "    import pyspark.sql.functions as F\n",
    "    import pyspark.sql.types as T\n",
    "if INPUT_FORMAT == 'koalas':\n",
    "    import databricks.koalas as ks\n",
    "elif INPUT_FORMAT == 'rdd':\n",
    "    import pyspark.mllib as M\n",
    "    from pyspark.mllib.feature import Word2Vec\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "    from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--py-files utilities.py,assignment2.py \\\n",
    "--deploy-mode client \\\n",
    "pyspark-shell'\n",
    "\n",
    "class args:\n",
    "    review_filename = data_cat.review_filename\n",
    "    product_filename = data_cat.product_filename\n",
    "    product_processed_filename = data_cat.product_processed_filename\n",
    "    ml_features_train_filename = data_cat.ml_features_train_filename\n",
    "    ml_features_test_filename = data_cat.ml_features_test_filename\n",
    "    output_root = '/home/{}/{}-pa2/test_results'.format(getpass.getuser(), PID)\n",
    "    test_results_root = data_cat.test_results_root\n",
    "    pid = PID\n",
    "\n",
    "pa2 = PA2Executor(args, input_format=INPUT_FORMAT)\n",
    "data_io = pa2.data_io\n",
    "data_dict = pa2.data_dict\n",
    "begin = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:52.353249Z",
     "start_time": "2020-01-26T20:45:52.343036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import your own dependencies\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, PCA\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql.functions import col, explode, mean, count, variance, when, size\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "#-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bring the part_1 datasets to memory and de-cache part_2 datasets. \n",
    "# Execute this once before you start working on this Part\n",
    "data_dict, _ = data_io.cache_switch(data_dict, 'part_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task0: warm up \n",
    "This task is provided for you to get familiar with Spark API. We will use the dataframe API to demonstrate. Solution is given to you and this task won't be graded.\n",
    "\n",
    "Refer to https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html for API guide.\n",
    "\n",
    "The task is to implement the function below. Given the ```product_data``` table:\n",
    "1. Take and print five rows.\n",
    "\n",
    "1. Select only the ```asin``` column, then print five rows of it.\n",
    "\n",
    "1. Select the row where ```asin = B00I8KEOTM``` and print it.\n",
    "\n",
    "1. Count the total number of rows.\n",
    "\n",
    "1. Calculate the mean ```price```.\n",
    "\n",
    "1. You need to conduct the above operations, then extract some statistics out of the generated columns. You need to put the statistics in a python dictionary named ```res```. The description and schema of it are as follows:\n",
    "    ```\n",
    "    res\n",
    "     | -- count_total: int -- count of total rows of the entire table after your operations\n",
    "     | -- mean_price: float -- mean value of column price\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:52.368918Z",
     "start_time": "2020-01-26T20:45:52.355018Z"
    }
   },
   "outputs": [],
   "source": [
    "def task_0(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    overall_column = 'overall'\n",
    "    # Outputs:\n",
    "    mean_rating_column = 'meanRating'\n",
    "    count_rating_column = 'countRating'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    product_data.show(5)\n",
    "    product_data[['asin']].show(5)\n",
    "    product_data.where(F.col('asin') == 'B00I8KEOTM').show()\n",
    "    count_rows = product_data.count()\n",
    "    mean_price = product_data.select(F.avg(F.col('price'))).head()[0]\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    # Calculate the values programmatically. Do not change the keys and do not\n",
    "    # hard-code values in the dict. Your submission will be evaluated with\n",
    "    # different inputs.\n",
    "    # Modify the values of the following dictionary accordingly.\n",
    "    res = {'count_total': None, 'mean_price': None}\n",
    "    \n",
    "    # Modify res:\n",
    "\n",
    "    res['count_total'] = count_rows\n",
    "    res['mean_price'] = mean_price\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:54.222111Z",
     "start_time": "2020-01-26T20:45:52.370377Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|      asin|           salesRank|          categories|               title|price|             related|\n",
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|B00I8HVV6E|{Home &amp; Kitch...|[[Home & Kitchen,...|Intelligent Desig...|27.99|{also_viewed -> [...|\n",
      "|B00I8KEOTM|                null|[[Apps for Androi...|                null| null|{also_viewed -> [...|\n",
      "|B00I8KCW4G|{Clothing -> 2233...|[[Clothing, Shoes...|eShakti Women's P...|41.95|{also_viewed -> [...|\n",
      "|B00I8JKCQW|{Clothing -> 1405...|[[Clothing, Shoes...|Lady Slimming Mid...| null|{also_viewed -> [...|\n",
      "|B00I8JKI8E|{Home &amp; Kitch...|[[Clothing, Shoes...|3 Tier Bangle Bra...|24.99|{also_viewed -> [...|\n",
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|      asin|\n",
      "+----------+\n",
      "|B00I8HVV6E|\n",
      "|B00I8KEOTM|\n",
      "|B00I8KCW4G|\n",
      "|B00I8JKCQW|\n",
      "|B00I8JKI8E|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 32:======================================================> (41 + 1) / 42]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "|      asin|salesRank|          categories|title|price|             related|\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "|B00I8KEOTM|     null|[[Apps for Androi...| null| null|{also_viewed -> [...|\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 36:=================================================>      (59 + 2) / 67]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_0 --------------------------------------------------------------\n",
      "2/2 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if INPUT_FORMAT == 'dataframe':\n",
    "    res = task_0(data_io, data_dict['product'])\n",
    "    pa2.tests.test(res, 'task_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:30:44.034299Z",
     "start_time": "2019-12-10T21:30:44.012787Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_1 assignment2.py\n",
    "def task_1(data_io, review_data, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    overall_column = 'overall'\n",
    "    # Outputs:\n",
    "    mean_rating_column = 'meanRating'\n",
    "    count_rating_column = 'countRating'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    # Aggregate review_data by asin\n",
    "    review_stats = review_data.groupBy(asin_column).agg(\n",
    "        F.mean(overall_column).alias(mean_rating_column),\n",
    "        F.count(overall_column).alias(count_rating_column)\n",
    "    )\n",
    "\n",
    "    # Left join with product_data to include all products\n",
    "    joined = product_data.select(asin_column).join(\n",
    "        review_stats, on=asin_column, how='left'\n",
    "    )\n",
    "\n",
    "    # Compute stats for res dict\n",
    "    count_total = joined.count()\n",
    "\n",
    "    mean_meanRating = joined.select(F.mean(mean_rating_column)).first()[0]\n",
    "    variance_meanRating = joined.select(F.variance(mean_rating_column)).first()[0]\n",
    "    numNulls_meanRating = joined.filter(F.col(mean_rating_column).isNull()).count()\n",
    "\n",
    "    mean_countRating = joined.select(F.mean(count_rating_column)).first()[0]\n",
    "    variance_countRating = joined.select(F.variance(count_rating_column)).first()[0]\n",
    "    numNulls_countRating = joined.filter(F.col(count_rating_column).isNull()).count()\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    # Calculate the values programmaticly. Do not change the keys and do not\n",
    "    # hard-code values in the dict. Your submission will be evaluated with\n",
    "    # different inputs.\n",
    "    # Modify the values of the following dictionary accordingly.\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_meanRating': None,\n",
    "        'variance_meanRating': None,\n",
    "        'numNulls_meanRating': None,\n",
    "        'mean_countRating': None,\n",
    "        'variance_countRating': None,\n",
    "        'numNulls_countRating': None\n",
    "    }\n",
    "    \n",
    "    # Modify res:\n",
    "    res = {\n",
    "        'count_total': int(count_total),\n",
    "        'mean_countRating': float(mean_countRating) if mean_countRating is not None else None,\n",
    "        'mean_meanRating': float(mean_meanRating) if mean_meanRating is not None else None,\n",
    "        'numNulls_countRating': int(numNulls_countRating),\n",
    "        'numNulls_meanRating': int(numNulls_meanRating),\n",
    "        'variance_countRating': float(variance_countRating) if variance_countRating is not None else None,\n",
    "        'variance_meanRating': float(variance_meanRating) if variance_meanRating is not None else None\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_1')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:19:04.214179Z",
     "start_time": "2019-12-09T22:18:39.293699Z"
    },
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_1 --------------------------------------------------------------\n",
      "Test 1/7 : count_total ... Pass\n",
      "Test 2/7 : mean_countRating ... Pass\n",
      "Test 3/7 : mean_meanRating ... Pass\n",
      "Test 4/7 : numNulls_countRating ... Pass\n",
      "Test 5/7 : numNulls_meanRating ... Pass\n",
      "Test 6/7 : variance_countRating ... Pass\n",
      "Test 7/7 : variance_meanRating ... Pass\n",
      "7/7 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_1(data_io, data_dict['review'], data_dict['product'])\n",
    "pa2.tests.test(res, 'task_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:16.942833Z",
     "start_time": "2019-12-10T21:31:16.925378Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_2 assignment2.py\n",
    "def task_2(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    salesRank_column = 'salesRank'\n",
    "    categories_column = 'categories'\n",
    "    asin_column = 'asin'\n",
    "    # Outputs:\n",
    "    category_column = 'category'\n",
    "    bestSalesCategory_column = 'bestSalesCategory'\n",
    "    bestSalesRank_column = 'bestSalesRank'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    # If possible, extract category[0][0]\n",
    "    category_col = F.when(\n",
    "        (F.col(categories_column).isNotNull()) &\n",
    "        (F.size(F.col(categories_column)) > 0) &\n",
    "        (F.col(categories_column)[0].isNotNull()) &\n",
    "        (F.size(F.col(categories_column)[0]) > 0) &\n",
    "        (F.col(categories_column)[0][0].isNotNull()) &\n",
    "        (F.col(categories_column)[0][0] != \"\"),\n",
    "        F.col(categories_column)[0][0]\n",
    "    ).otherwise(None).alias(category_column)\n",
    "\n",
    "\n",
    "    # Extract bestSalesCategory and bestSalesRank from salesRank map\n",
    "    best_sales_cat = F.when(\n",
    "        F.col(salesRank_column).isNotNull() & (F.size(F.map_keys(F.col(salesRank_column))) > 0),\n",
    "        F.map_keys(F.col(salesRank_column))[0]\n",
    "    ).otherwise(None).alias(bestSalesCategory_column)\n",
    "\n",
    "    best_sales_rank = F.when(\n",
    "        F.col(salesRank_column).isNotNull() & (F.size(F.map_values(F.col(salesRank_column))) > 0),\n",
    "        F.map_values(F.col(salesRank_column))[0]\n",
    "    ).otherwise(None).alias(bestSalesRank_column)\n",
    "\n",
    "    # Add new columns\n",
    "    df = product_data.select(\n",
    "        F.col(asin_column),\n",
    "        category_col,\n",
    "        best_sales_cat,\n",
    "        best_sales_rank\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_bestSalesRank': None,\n",
    "        'variance_bestSalesRank': None,\n",
    "        'numNulls_category': None,\n",
    "        'countDistinct_category': None,\n",
    "        'numNulls_bestSalesCategory': None,\n",
    "        'countDistinct_bestSalesCategory': None\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "    res = {\n",
    "        'count_total': int(df.count()),\n",
    "        'mean_bestSalesRank': float(df.select(F.mean(bestSalesRank_column)).first()[0]) \\\n",
    "            if df.select(F.mean(bestSalesRank_column)).first()[0] is not None else None,\n",
    "        'variance_bestSalesRank': float(df.select(F.variance(bestSalesRank_column)).first()[0]) \\\n",
    "            if df.select(F.variance(bestSalesRank_column)).first()[0] is not None else None,\n",
    "        'numNulls_category': int(df.filter(F.col(category_column).isNull()).count()),\n",
    "        'countDistinct_category': int(df.select(category_column).distinct().filter(F.col(category_column).isNotNull()).count()),\n",
    "        'numNulls_bestSalesCategory': int(df.filter(F.col(bestSalesCategory_column).isNull()).count()),\n",
    "        'countDistinct_bestSalesCategory': int(df.select(bestSalesCategory_column).distinct().filter(F.col(bestSalesCategory_column).isNotNull()).count())\n",
    "    }\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_2')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:19:19.308187Z",
     "start_time": "2019-12-09T22:19:04.274345Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_2 --------------------------------------------------------------\n",
      "Test 1/7 : countDistinct_bestSalesCategory ... Pass\n",
      "Test 2/7 : countDistinct_category ... Pass\n",
      "Test 3/7 : count_total ... Pass\n",
      "Test 4/7 : mean_bestSalesRank ... Pass\n",
      "Test 5/7 : numNulls_bestSalesCategory ... Pass\n",
      "Test 6/7 : numNulls_category ... Pass\n",
      "Test 7/7 : variance_bestSalesRank ... Pass\n",
      "7/7 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_2(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:26.542481Z",
     "start_time": "2019-12-10T21:31:26.525050Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_3 assignment2.py\n",
    "def task_3(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    price_column = 'price'\n",
    "    attribute = 'also_viewed'\n",
    "    related_column = 'related'\n",
    "    # Outputs:\n",
    "    meanPriceAlsoViewed_column = 'meanPriceAlsoViewed'\n",
    "    countAlsoViewed_column = 'countAlsoViewed'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    # Explode also_viewed\n",
    "    exploded = product_data.select(\n",
    "        col(asin_column).alias('main_asin'),\n",
    "        col(f'{related_column}.{attribute}').alias('related_asins')\n",
    "    ).withColumn(\"related_asin\", explode(\"related_asins\"))\n",
    "\n",
    "    # Join to get price of related products\n",
    "    joined = exploded.join(\n",
    "        product_data.select(col(asin_column).alias('related_asin'), col(price_column)),\n",
    "        on='related_asin',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Filter out rows where price is null\n",
    "    valid_prices = joined.filter(col(price_column).isNotNull())\n",
    "\n",
    "    # Compute meanPriceAlsoViewed per main product\n",
    "    mean_price_df = valid_prices.groupBy('main_asin').agg(\n",
    "        mean(price_column).alias(meanPriceAlsoViewed_column)\n",
    "    )\n",
    "\n",
    "    count_df = product_data.select(\n",
    "        col(asin_column).alias('main_asin'),\n",
    "        col(f'{related_column}.{attribute}').alias('related_asins')\n",
    "    ).withColumn(\n",
    "        countAlsoViewed_column,\n",
    "        when(\n",
    "            (col('related_asins').isNull()) | (size(col('related_asins')) == 0),\n",
    "            None\n",
    "        ).otherwise(size(col('related_asins')).cast(FloatType()))\n",
    "    ).select('main_asin', countAlsoViewed_column)\n",
    "\n",
    "    \n",
    "    # Join mean and count\n",
    "    summary_df = product_data.select(col(asin_column).alias('main_asin')) \\\n",
    "        .join(mean_price_df, on='main_asin', how='left') \\\n",
    "        .join(count_df, on='main_asin', how='left')\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_meanPriceAlsoViewed': None,\n",
    "        'variance_meanPriceAlsoViewed': None,\n",
    "        'numNulls_meanPriceAlsoViewed': None,\n",
    "        'mean_countAlsoViewed': None,\n",
    "        'variance_countAlsoViewed': None,\n",
    "        'numNulls_countAlsoViewed': None\n",
    "    }\n",
    "    # Modify res:\n",
    "    \n",
    "    res = summary_df.agg(\n",
    "        count(\"*\").alias(\"count_total\"),\n",
    "        mean(meanPriceAlsoViewed_column).alias(\"mean_meanPriceAlsoViewed\"),\n",
    "        variance(meanPriceAlsoViewed_column).alias(\"variance_meanPriceAlsoViewed\"),\n",
    "        count(when(col(meanPriceAlsoViewed_column).isNull(), 1)).alias(\"numNulls_meanPriceAlsoViewed\"),\n",
    "        mean(countAlsoViewed_column).alias(\"mean_countAlsoViewed\"),\n",
    "        variance(countAlsoViewed_column).alias(\"variance_countAlsoViewed\"),\n",
    "        count(when(col(countAlsoViewed_column).isNull(), 1)).alias(\"numNulls_countAlsoViewed\")\n",
    "    ).first().asDict()\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_3')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:20:41.442745Z",
     "start_time": "2019-12-09T22:19:19.358780Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 157:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_3 --------------------------------------------------------------\n",
      "Test 1/7 : count_total ... Pass\n",
      "Test 2/7 : mean_countAlsoViewed ... Pass\n",
      "Test 3/7 : mean_meanPriceAlsoViewed ... Pass\n",
      "Test 4/7 : numNulls_countAlsoViewed ... Pass\n",
      "Test 5/7 : numNulls_meanPriceAlsoViewed ... Pass\n",
      "Test 6/7 : variance_countAlsoViewed ... Pass\n",
      "Test 7/7 : variance_meanPriceAlsoViewed ... Pass\n",
      "7/7 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_3(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:39.503390Z",
     "start_time": "2019-12-10T21:31:39.484724Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_4 assignment2.py\n",
    "def task_4(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    price_column = 'price'\n",
    "    title_column = 'title'\n",
    "    # Outputs:\n",
    "    meanImputedPrice_column = 'meanImputedPrice'\n",
    "    medianImputedPrice_column = 'medianImputedPrice'\n",
    "    unknownImputedTitle_column = 'unknownImputedTitle'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    # Cast price to float\n",
    "    product_data = product_data.withColumn(price_column, F.col(price_column).cast(T.FloatType()))\n",
    "\n",
    "    # Compute mean and median of non-null prices\n",
    "    price_stats = product_data.select(price_column).filter(F.col(price_column).isNotNull())\n",
    "    price_mean = price_stats.agg(F.mean(price_column)).first()[0]\n",
    "    price_median = price_stats.approxQuantile(price_column, [0.5], 0.01)[0]\n",
    "\n",
    "    # Mean imputation\n",
    "    with_mean = product_data.withColumn(\n",
    "        meanImputedPrice_column,\n",
    "        F.when(F.col(price_column).isNotNull(), F.col(price_column)).otherwise(F.lit(price_mean))\n",
    "    )\n",
    "\n",
    "    # Median imputation\n",
    "    with_both = with_mean.withColumn(\n",
    "        medianImputedPrice_column,\n",
    "        F.when(F.col(price_column).isNotNull(), F.col(price_column)).otherwise(F.lit(price_median))\n",
    "    )\n",
    "\n",
    "    # Title imputation (nulls or empty strings set to 'unknown')\n",
    "    with_all = with_both.withColumn(\n",
    "        unknownImputedTitle_column,\n",
    "        F.when(F.col(title_column).isNull() | (F.trim(F.col(title_column)) == ''), F.lit('unknown'))\n",
    "         .otherwise(F.col(title_column))\n",
    "    )\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'mean_meanImputedPrice': None,\n",
    "        'variance_meanImputedPrice': None,\n",
    "        'numNulls_meanImputedPrice': None,\n",
    "        'mean_medianImputedPrice': None,\n",
    "        'variance_medianImputedPrice': None,\n",
    "        'numNulls_medianImputedPrice': None,\n",
    "        'numUnknowns_unknownImputedTitle': None\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "    res = {\n",
    "        'count_total': with_all.count(),\n",
    "        'mean_meanImputedPrice': with_all.select(F.mean(meanImputedPrice_column)).first()[0],\n",
    "        'variance_meanImputedPrice': with_all.select(F.variance(meanImputedPrice_column)).first()[0],\n",
    "        'numNulls_meanImputedPrice': with_all.filter(F.col(meanImputedPrice_column).isNull()).count(),\n",
    "        'mean_medianImputedPrice': with_all.select(F.mean(medianImputedPrice_column)).first()[0],\n",
    "        'variance_medianImputedPrice': with_all.select(F.variance(medianImputedPrice_column)).first()[0],\n",
    "        'numNulls_medianImputedPrice': with_all.filter(F.col(medianImputedPrice_column).isNull()).count(),\n",
    "        'numUnknowns_unknownImputedTitle': with_all.filter(F.col(unknownImputedTitle_column) == 'unknown').count()\n",
    "    }\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_4')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:20:47.953226Z",
     "start_time": "2019-12-09T22:20:41.523379Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 187:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_4 --------------------------------------------------------------\n",
      "Test 1/8 : count_total ... Pass\n",
      "Test 2/8 : mean_meanImputedPrice ... Pass\n",
      "Test 3/8 : mean_medianImputedPrice ... Pass\n",
      "Test 4/8 : numNulls_meanImputedPrice ... Pass\n",
      "Test 5/8 : numNulls_medianImputedPrice ... Pass\n",
      "Test 6/8 : numUnknowns_unknownImputedTitle ... Pass\n",
      "Test 7/8 : variance_meanImputedPrice ... Pass\n",
      "Test 8/8 : variance_medianImputedPrice ... Pass\n",
      "8/8 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_4(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:32:29.284661Z",
     "start_time": "2019-12-10T21:32:29.267237Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_5 assignment2.py\n",
    "def task_5(data_io, product_processed_data, word_0, word_1, word_2):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    title_column = 'title'\n",
    "    # Outputs:\n",
    "    titleArray_column = 'titleArray'\n",
    "    titleVector_column = 'titleVector'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    SEED = 102\n",
    "    \n",
    "    # Lowercase and split title\n",
    "    product_processed_data_output = product_processed_data.withColumn(\n",
    "        titleArray_column,\n",
    "        F.split(F.lower(F.col(title_column)), ' ')\n",
    "    )\n",
    "\n",
    "    # Train Word2Vec\n",
    "    word2vec = M.feature.Word2Vec(\n",
    "        inputCol=titleArray_column,\n",
    "        outputCol=titleVector_column,\n",
    "        vectorSize=16,\n",
    "        minCount=100,\n",
    "        numPartitions=4,\n",
    "        seed=SEED\n",
    "    )\n",
    "    model = word2vec.fit(product_processed_data_output)\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'size_vocabulary': None,\n",
    "        'word_0_synonyms': [(None, None), ],\n",
    "        'word_1_synonyms': [(None, None), ],\n",
    "        'word_2_synonyms': [(None, None), ]\n",
    "    }\n",
    "    # Modify res:\n",
    "    res['count_total'] = product_processed_data_output.count()\n",
    "    res['size_vocabulary'] = model.getVectors().count()\n",
    "    for name, word in zip(\n",
    "        ['word_0_synonyms', 'word_1_synonyms', 'word_2_synonyms'],\n",
    "        [word_0, word_1, word_2]\n",
    "    ):\n",
    "        res[name] = model.findSynonymsArray(word, 10)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_5')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:26:05.015529Z",
     "start_time": "2019-12-09T22:20:47.999834Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/29 02:15:33 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/29 02:15:33 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "tests for task_5 --------------------------------------------------------------\n",
      "Test 1/8 : count_total ... Pass\n",
      "Test 2/8 : size_vocabulary ... Pass\n",
      "Test 3/8 : word_0_synonyms-length ... Pass\n",
      "Test 4/8 : word_0_synonyms-correctness ... Pass\n",
      "Test 5/8 : word_1_synonyms-length ... Pass\n",
      "Test 6/8 : word_1_synonyms-correctness ... Pass\n",
      "Test 7/8 : word_2_synonyms-length ... Pass\n",
      "Test 8/8 : word_2_synonyms-correctness ... Pass\n",
      "5/5 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_5(data_io, data_dict['product_processed'], 'piano', 'rice', 'laptop')\n",
    "pa2.tests.test(res, 'task_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:32:39.991460Z",
     "start_time": "2019-12-10T21:32:39.974136Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_6 assignment2.py\n",
    "def task_6(data_io, product_processed_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    category_column = 'category'\n",
    "    # Outputs:\n",
    "    categoryIndex_column = 'categoryIndex'\n",
    "    categoryOneHot_column = 'categoryOneHot'\n",
    "    categoryPCA_column = 'categoryPCA'\n",
    "    # -------------------------------------------------------------------------    \n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    # StringIndexer\n",
    "    indexer = StringIndexer(inputCol=category_column, outputCol=categoryIndex_column, handleInvalid='keep')\n",
    "    indexed_data = indexer.fit(product_processed_data).transform(product_processed_data)\n",
    "\n",
    "    # OneHotEncoder\n",
    "    encoder = OneHotEncoder(inputCols=[categoryIndex_column],\n",
    "                            outputCols=[categoryOneHot_column],\n",
    "                            dropLast=True)\n",
    "    encoded_data = encoder.fit(indexed_data).transform(indexed_data)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(k=15, inputCol=categoryOneHot_column, outputCol=categoryPCA_column)\n",
    "    pca_model = pca.fit(encoded_data)\n",
    "    transformed_data = pca_model.transform(encoded_data)\n",
    "\n",
    "    # Summarizer mean vectors \n",
    "    summarizer = Summarizer.metrics(\"mean\")\n",
    "\n",
    "    summary_onehot = transformed_data.select(Summarizer.mean(F.col(categoryOneHot_column)).alias(\"mean\")).collect()[0][\"mean\"]\n",
    "    summary_pca = transformed_data.select(Summarizer.mean(F.col(categoryPCA_column)).alias(\"mean\")).collect()[0][\"mean\"]\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'meanVector_categoryOneHot': [None, ],\n",
    "        'meanVector_categoryPCA': [None, ]\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "    res = {\n",
    "        'count_total': transformed_data.count(),\n",
    "        'meanVector_categoryOneHot': summary_onehot.toArray().tolist(),\n",
    "        'meanVector_categoryPCA': summary_pca.toArray().tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_6')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:29:57.717617Z",
     "start_time": "2019-12-09T22:29:51.132434Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/05/29 02:15:56 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "25/05/29 02:15:56 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_6 --------------------------------------------------------------\n",
      "Test 1/9 : count_total ... Pass\n",
      "Test 2/9 : meanVector_categoryOneHot-length ... Pass\n",
      "Test 3/9 : meanVector_categoryOneHot-sum ... Pass\n",
      "Test 4/9 : meanVector_categoryOneHot-mean ... Pass\n",
      "Test 5/9 : meanVector_categoryOneHot-variance ... Pass\n",
      "Test 6/9 : meanVector_categoryPCA-length ... Pass\n",
      "Test 7/9 : meanVector_categoryPCA-sum ... Pass\n",
      "Test 8/9 : meanVector_categoryPCA-mean ... Pass\n",
      "Test 9/9 : meanVector_categoryPCA-variance ... Pass\n",
      "9/9 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_6(data_io, data_dict['product_processed'])\n",
    "pa2.tests.test(res, 'task_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:23:18.882119Z",
     "start_time": "2019-11-26T21:23:18.873162Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End to end time: 686.8751242160797\n"
     ]
    }
   ],
   "source": [
    "print (\"End to end time: {}\".format(time.time()-begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bring the part_2 datasets to memory and de-cache part_1 datasets.\n",
    "# Execute this once before you start working on this Part\n",
    "data_dict, _ = data_io.cache_switch(data_dict, 'part_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_7(data_io, train_data, test_data):\n",
    "    \n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    \n",
    "    # Decision Tree Regressor with maxDepth=5\n",
    "    dt = M.regression.DecisionTreeRegressor(featuresCol='features', labelCol='overall', maxDepth=5)\n",
    "\n",
    "    # Fit model on training data\n",
    "    model = dt.fit(train_data)\n",
    "\n",
    "    # Predict on test data\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Evaluate using RMSE\n",
    "    evaluator = M.evaluation.RegressionEvaluator(labelCol='overall', predictionCol='prediction', metricName='rmse')\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'test_rmse': None\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "    res = {\n",
    "        'test_rmse': rmse\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_7')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_7 --------------------------------------------------------------\n",
      "Test 1/1 : test_rmse ... Pass\n",
      "1/1 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_7(data_io, data_dict['ml_features_train'], data_dict['ml_features_test'])\n",
    "pa2.tests.test(res, 'task_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_8(data_io, train_data, test_data):\n",
    "    \n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    # Split original training data into new training and validation sets\n",
    "    train_split, valid_split = train_data.randomSplit([0.75, 0.25], seed=102)\n",
    "\n",
    "    # Define evaluator\n",
    "    evaluator = M.evaluation.RegressionEvaluator(labelCol='overall', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "    # Loop through each maxDepth value\n",
    "    depths = [5, 7, 9, 12]\n",
    "    valid_rmses = {}\n",
    "\n",
    "    best_model = None\n",
    "    best_depth = None\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    for depth in depths:\n",
    "        dt = M.regression.DecisionTreeRegressor(featuresCol='features', labelCol='overall', maxDepth=depth)\n",
    "        model = dt.fit(train_split)\n",
    "        predictions = model.transform(valid_split)\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        valid_rmses[depth] = rmse\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = model\n",
    "            best_depth = depth\n",
    "\n",
    "    # Evaluate best model on test data\n",
    "    test_predictions = best_model.transform(test_data)\n",
    "    test_rmse = evaluator.evaluate(test_predictions)\n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'test_rmse': None,\n",
    "        'valid_rmse_depth_5': None,\n",
    "        'valid_rmse_depth_7': None,\n",
    "        'valid_rmse_depth_9': None,\n",
    "        'valid_rmse_depth_12': None,\n",
    "    }\n",
    "    # Modify res:\n",
    "    res = {\n",
    "        'test_rmse': test_rmse,\n",
    "        'valid_rmse_depth_5': valid_rmses[5],\n",
    "        'valid_rmse_depth_7': valid_rmses[7],\n",
    "        'valid_rmse_depth_9': valid_rmses[9],\n",
    "        'valid_rmse_depth_12': valid_rmses[12],\n",
    "    }\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_8')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_8 --------------------------------------------------------------\n",
      "Test 1/5 : test_rmse ... Pass\n",
      "Test 2/5 : valid_rmse_depth_12 ... Pass\n",
      "Test 3/5 : valid_rmse_depth_5 ... Pass\n",
      "Test 4/5 : valid_rmse_depth_7 ... Pass\n",
      "Test 5/5 : valid_rmse_depth_9 ... Pass\n",
      "5/5 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_8(data_io, data_dict['ml_features_train'], data_dict['ml_features_test'])\n",
    "pa2.tests.test(res, 'task_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End to end time: 1166.121743440628\n"
     ]
    }
   ],
   "source": [
    "print (\"End to end time: {}\".format(time.time()-begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
