{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285a4873-ff33-4ff1-b7d2-0032254484fe",
   "metadata": {},
   "source": [
    "## <font color='red'> INSTRUCTIONS </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89957ed8-c2d1-4592-8821-88806390d1cc",
   "metadata": {},
   "source": [
    "<b> \n",
    "1. Write your code only in cells below the \"WRITE CODE BELOW\" title. Do not modify the code below the \"DO NOT MODIFY\" title. <br>\n",
    "2. The expected data types of the output answers for each question are given in the last cell through assertion statements. Your answers must match these expected output data types. Hint: Many of the answers need to be a Python dictionary. Consider methods like to_dict() to convert a Pandas Series to a dictionary. <br>\n",
    "3. The answers are then written to a JSON file named my_results_PA1.json. You can compare this with the provided expected output file \"expected_results_PA1.json\". <br>\n",
    "4. After you complete writing your code, click \"Kernel -> Restart Kernel and Run All Cells\" on the top toolbar. There should NOT be any syntax/runtime errors, otherwise points will be deducted. <br>\n",
    "5. For submitting your solution, first download your notebook by clicking \"File -> Download\". Rename the file as &ltTEAM_ID&gt.ipynb\" and upload to Canvas.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f7e94-c5b1-494c-8aab-832242527a4e",
   "metadata": {},
   "source": [
    "## <font color='red'> DO NOT MODIFY </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76f3c8d7-690f-428b-982d-94265b4a7f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://172.31.33.85:8786' processes=16 threads=16, memory=62.46 GiB>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from dask.distributed import Client\n",
    "import ctypes\n",
    "import numpy as np\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    \"\"\"\n",
    "    helps to fix any memory leaks.\n",
    "    \"\"\"\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)\n",
    "\n",
    "client = Client(\"127.0.0.1:8786\")\n",
    "client.run(trim_memory)\n",
    "client.restart()\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb6ac532-d64f-4659-9cc8-94481f48c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b6eb9-e5d7-423a-a0bc-7b86e6db1ab4",
   "metadata": {},
   "source": [
    "## <font color='blue'> WRITE CODE BELOW </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aca2064f-c35c-4a9c-a866-63f0d7e60a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "from dask.distributed import as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf416f4d-1782-4fa5-9b2b-b44aafd55934",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in the 'user_reviews.csv' and 'products.csv' files, perform your calculations and place the answers in variables ans1 - ans7.\n",
    "# Read user_reviews.csv with specified blocksize\n",
    "user_reviews = dd.read_csv(\n",
    "    'user_reviews.csv',\n",
    "    blocksize='64MB', \n",
    ")\n",
    "\n",
    "# Read products.csv with specified blocksize\n",
    "products = dd.read_csv(\n",
    "    'products.csv',\n",
    "    blocksize='64MB',\n",
    "    dtype={'asin': 'object'}\n",
    ")\n",
    "\n",
    "# Q1: Percentage of missing values in user_reviews\n",
    "ans1 = (user_reviews.isnull().sum() / user_reviews.shape[0]).compute() * 100\n",
    "ans1 = ans1.round(2).to_dict()\n",
    "\n",
    "# Q2: Percentage of missing values in products\n",
    "ans2 = (products.isnull().sum() / products.shape[0]).compute() * 100\n",
    "ans2 = ans2.round(2).to_dict()\n",
    "\n",
    "\n",
    "# Q3: Pearson correlation between price and overall rating\n",
    "products['price'] = dd.to_numeric(products['price'], errors='coerce')\n",
    "user_reviews['overall'] = dd.to_numeric(user_reviews['overall'], errors='coerce')\n",
    "\n",
    "# Only selecting the columns needed for merging and correlation\n",
    "merged = user_reviews[['asin', 'overall']].merge(\n",
    "                    products[['asin', 'price']], on='asin', how='inner'\n",
    "                    ).dropna(subset=['price', 'overall']).persist()\n",
    "\n",
    "correlation_matrix = merged[['price', 'overall']].corr().compute()\n",
    "ans3 = correlation_matrix.loc['price', 'overall'].round(2)\n",
    "ans3 = float(ans3)\n",
    "\n",
    "# Q4: Price statistics in products\n",
    "price_clean = products['price'].persist()\n",
    "\n",
    "# Compute summary stats\n",
    "price_stats = price_clean.describe().compute()\n",
    "# Convert to plain float\n",
    "\n",
    "ans4 = {\n",
    "    'mean': float(price_stats['mean']),\n",
    "    'std': float(price_stats['std']),\n",
    "    'min': float(price_stats['min']),\n",
    "    'max': float(price_stats['max']),\n",
    "    'median': float(price_stats['50%'])\n",
    "}\n",
    "\n",
    "# Q5: Number of products per super-category (sorted descending)\n",
    "def extract_super_category(cat_string):\n",
    "    try:\n",
    "        categories_list = ast.literal_eval(cat_string)\n",
    "        if isinstance(categories_list, list) and len(categories_list) > 0:\n",
    "            return categories_list[0][0]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "products['super_category'] = products['categories'].map(extract_super_category, meta=('super_category', str))\n",
    "super_category_counts = products['super_category'].dropna().value_counts().compute()\n",
    "\n",
    "# Sort by counts descending\n",
    "super_category_counts = super_category_counts.sort_values(ascending=False)\n",
    "\n",
    "# Remove empty category keys\n",
    "if '' in super_category_counts:\n",
    "    super_category_counts = super_category_counts.drop('')\n",
    "\n",
    "ans5 = super_category_counts.to_dict()\n",
    "\n",
    "# # Q6: 152s, 162s (time with only Q6)\n",
    "# reviews_unique = user_reviews.drop_duplicates()\n",
    "# products_unique = products.drop_duplicates()\n",
    "\n",
    "# dangling = reviews_unique.merge(products_unique, on='asin', how='left', indicator=True)\n",
    "# dangling_only = dangling[dangling['_merge'] == 'left_only']\n",
    "\n",
    "# # dangling = dangling[dangling['_merge'] == 'left_only']\n",
    "# # has_dangling = dangling.shape[0] > 0\n",
    "# # ans6 = int(has_dangling.compute())\n",
    "\n",
    "# ans6 = int(dangling_only.shape[0].compute() > 0) \n",
    "\n",
    "# Q6: 140s?(idk how i got that), 207s, 206s  (time with only Q6)\n",
    "reviews_unique = user_reviews.drop_duplicates()\n",
    "products_unique = products.drop_duplicates()\n",
    "\n",
    "merged = reviews_unique.merge(products_unique, on='asin', how='left', indicator=True)\n",
    "dangling_only = merged[merged['_merge'] == 'left_only']\n",
    "\n",
    "# convert partitions to delayed objects\n",
    "delayed_parts = dangling_only.to_delayed()\n",
    "\n",
    "# force computation first\n",
    "@delayed\n",
    "def is_nonempty_delayed(df):\n",
    "    return not df.empty\n",
    "\n",
    "# submit delayed computations to client\n",
    "futures = [client.compute(is_nonempty_delayed(part)) for part in delayed_parts]\n",
    "\n",
    "# use as_completed to short-circuit\n",
    "found = False\n",
    "for future in as_completed(futures):\n",
    "    if future.result():\n",
    "        found = True\n",
    "        break\n",
    "        \n",
    "# to stop the workers in the background (idk if theres a better way to do this)\n",
    "client.close()\n",
    "\n",
    "ans6 = int(found)\n",
    "\n",
    "ans7 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d92954-28b3-4ad0-b7de-d8b8f4816c80",
   "metadata": {},
   "source": [
    "## <font color='red'> DO NOT MODIFY </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c438177d-8c4d-4871-bbc6-bea2f0a004b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0adca53b-b276-4297-8434-6c0e94810d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time = 383.7713415622711s\n"
     ]
    }
   ],
   "source": [
    "print(f\"execution time = {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935be195-dcc9-4e97-911a-bae25e2a70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "assert type(ans1) == dict, f\"answer to question 1 must be a dictionary like {{'reviewerID':0.2, ..}}, got type = {type(ans1)}\"\n",
    "assert type(ans2) == dict, f\"answer to question 2 must be a dictionary like {{'asin':0.2, ..}}, got type = {type(ans2)}\"\n",
    "assert type(ans3) == float, f\"answer to question 3 must be a float like 0.8, got type = {type(ans3)}\"\n",
    "assert type(ans4) == dict, f\"answer to question 4 must be a dictionary like {{'mean':0.4,'max':0.6,'median':0.6...}}, got type = {type(ans4)}\"\n",
    "assert type(ans5) == dict, f\"answer to question 5 must be a dictionary, got type = {type(ans5)}\"         \n",
    "assert ans6 == 0 or ans6==1, f\"answer to question 6 must be 0 or 1, got value = {ans6}\" \n",
    "assert ans7 == 0 or ans7==1, f\"answer to question 7 must be 0 or 1, got value = {ans7}\" \n",
    "\n",
    "ans_dict = {\n",
    "    \"q1\": ans1,\n",
    "    \"q2\": ans2,\n",
    "    \"q3\": ans3,\n",
    "    \"q4\": ans4,\n",
    "    \"q5\": ans5,\n",
    "    \"q6\": ans6,\n",
    "    \"q7\": ans7,\n",
    "    \"runtime\": end-start\n",
    "}\n",
    "with open('my_results_PA1.json', 'w') as outfile: json.dump(ans_dict, outfile)         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
