{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285a4873-ff33-4ff1-b7d2-0032254484fe",
   "metadata": {},
   "source": [
    "## <font color='red'> INSTRUCTIONS </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89957ed8-c2d1-4592-8821-88806390d1cc",
   "metadata": {},
   "source": [
    "<b> \n",
    "1. Write your code only in cells below the \"WRITE CODE BELOW\" title. Do not modify the code below the \"DO NOT MODIFY\" title. <br>\n",
    "2. The expected data types of the output answers for each question are given in the last cell through assertion statements. Your answers must match these expected output data types. Hint: Many of the answers need to be a Python dictionary. Consider methods like to_dict() to convert a Pandas Series to a dictionary. <br>\n",
    "3. The answers are then written to a JSON file named my_results_PA1.json. You can compare this with the provided expected output file \"expected_results_PA1.json\". <br>\n",
    "4. After you complete writing your code, click \"Kernel -> Restart Kernel and Run All Cells\" on the top toolbar. There should NOT be any syntax/runtime errors, otherwise points will be deducted. <br>\n",
    "5. For submitting your solution, first download your notebook by clicking \"File -> Download\". Rename the file as &ltTEAM_ID&gt.ipynb\" and upload to Canvas.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5f7e94-c5b1-494c-8aab-832242527a4e",
   "metadata": {},
   "source": [
    "## <font color='red'> DO NOT MODIFY </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f3c8d7-690f-428b-982d-94265b4a7f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://172.31.43.101:8786' processes=8 threads=8, memory=31.23 GiB>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from dask.distributed import Client\n",
    "import ctypes\n",
    "import numpy as np\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    \"\"\"\n",
    "    helps to fix any memory leaks.\n",
    "    \"\"\"\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)\n",
    "\n",
    "client = Client(\"127.0.0.1:8786\")\n",
    "client.run(trim_memory)\n",
    "client.restart()\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6ac532-d64f-4659-9cc8-94481f48c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b6eb9-e5d7-423a-a0bc-7b86e6db1ab4",
   "metadata": {},
   "source": [
    "## <font color='blue'> WRITE CODE BELOW </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf416f4d-1782-4fa5-9b2b-b44aafd55934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dask_env/lib/python3.10/site-packages/dask/dataframe/multi.py:169: UserWarning: Merging dataframes with merge column data type mismatches: \n",
      "+------------------+------------+-------------+\n",
      "| Merge columns    | left dtype | right dtype |\n",
      "+------------------+------------+-------------+\n",
      "| ('asin', 'asin') | object     | string      |\n",
      "+------------------+------------+-------------+\n",
      "Cast dtypes explicitly to avoid unexpected results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Read user_reviews.csv with specified blocksize\n",
    "user_reviews = dd.read_csv(\n",
    "    'user_reviews.csv',\n",
    "    blocksize='64MB', \n",
    ")\n",
    "\n",
    "# Read products.csv with specified blocksize\n",
    "products = dd.read_csv(\n",
    "    'products.csv',\n",
    "    blocksize='64MB',\n",
    "    dtype={'asin': 'object'}\n",
    ")\n",
    "\n",
    "# Q1: Percentage of missing values in user_reviews\n",
    "ans1 = (user_reviews.isnull().sum() / user_reviews.shape[0]).compute() * 100\n",
    "ans1 = ans1.round(2).to_dict()\n",
    "\n",
    "# Q2: Percentage of missing values in products\n",
    "ans2 = (products.isnull().sum() / products.shape[0]).compute() * 100\n",
    "ans2 = ans2.round(2).to_dict()\n",
    "\n",
    "\n",
    "# Q3: Pearson correlation between price and overall rating\n",
    "products['price'] = dd.to_numeric(products['price'], errors='coerce')\n",
    "user_reviews['overall'] = dd.to_numeric(user_reviews['overall'], errors='coerce')\n",
    "\n",
    "# Only selecting the columns needed for merging and correlation\n",
    "merged = user_reviews[['asin', 'overall']].merge(\n",
    "                    products[['asin', 'price']], on='asin', how='inner'\n",
    "                    ).dropna(subset=['price', 'overall']).persist()\n",
    "\n",
    "correlation_matrix = merged[['price', 'overall']].corr().compute()\n",
    "ans3 = correlation_matrix.loc['price', 'overall'].round(2)\n",
    "ans3 = float(ans3)\n",
    "\n",
    "# Q4: Price statistics in products\n",
    "price_clean = products['price'].persist()\n",
    "\n",
    "# Compute summary stats\n",
    "price_stats = price_clean.describe().compute()\n",
    "# Convert to plain float\n",
    "\n",
    "ans4 = {\n",
    "    'mean': float(price_stats['mean']),\n",
    "    'std': float(price_stats['std']),\n",
    "    'min': float(price_stats['min']),\n",
    "    'max': float(price_stats['max']),\n",
    "    'median': float(price_stats['50%'])\n",
    "}\n",
    "\n",
    "# Q5: Number of products per super-category (sorted descending)\n",
    "def extract_super_category(cat_string):\n",
    "    try:\n",
    "        categories_list = ast.literal_eval(cat_string)\n",
    "        if isinstance(categories_list, list) and len(categories_list) > 0:\n",
    "            return categories_list[0][0]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "products['super_category'] = products['categories'].map(extract_super_category, meta=('super_category', str))\n",
    "super_category_counts = products['super_category'].dropna().value_counts().compute()\n",
    "\n",
    "# Sort by counts descending\n",
    "super_category_counts = super_category_counts.sort_values(ascending=False)\n",
    "\n",
    "# Remove empty category keys\n",
    "if '' in super_category_counts:\n",
    "    super_category_counts = super_category_counts.drop('')\n",
    "\n",
    "ans5 = super_category_counts.to_dict()\n",
    "\n",
    "# Q6: Dangling reference check from product ids in \"related\" column to \"asin\" column of product table\n",
    "review_asins = user_reviews[['asin']].dropna().drop_duplicates()\n",
    "product_asins = products[['asin']].dropna().drop_duplicates().persist()\n",
    "\n",
    "# Perform left merge to find dangling references\n",
    "dangling = review_asins.merge(product_asins, on='asin', how='left', indicator=True)\n",
    "dangling_only = dangling[dangling['_merge'] == 'left_only']\n",
    "\n",
    "# Check if there are any dangling references\n",
    "ans6 = int(dangling_only.shape[0].compute() > 0)\n",
    "\n",
    "# Q7: Dangling reference check from product ids in \"related\" column to \"asin\" column of product table\n",
    "def extract_related_asins(related):\n",
    "    if isinstance(related, str):\n",
    "        try:\n",
    "            related = eval(related)  # evaluate string to dict\n",
    "        except:\n",
    "            return []\n",
    "    if isinstance(related, dict):\n",
    "        return [item for sublist in related.values() for item in sublist]\n",
    "    return []\n",
    "\n",
    "related_asins = (\n",
    "    products['related']\n",
    "    .dropna()\n",
    "    .map(extract_related_asins, meta=('related', 'object'))\n",
    "    .explode()\n",
    "    .dropna()\n",
    "    .to_frame(name='asin')\n",
    "    .drop_duplicates()\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "dangling = related_asins.merge(product_asins, on='asin', how='left', indicator=True)\n",
    "dangling_only = dangling[dangling['_merge'] == 'left_only']\n",
    "\n",
    "ans7 = int(dangling_only.shape[0].compute() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d92954-28b3-4ad0-b7de-d8b8f4816c80",
   "metadata": {},
   "source": [
    "## <font color='red'> DO NOT MODIFY </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c438177d-8c4d-4871-bbc6-bea2f0a004b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0adca53b-b276-4297-8434-6c0e94810d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution time = 298.9169328212738s\n"
     ]
    }
   ],
   "source": [
    "print(f\"execution time = {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935be195-dcc9-4e97-911a-bae25e2a70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "assert type(ans1) == dict, f\"answer to question 1 must be a dictionary like {{'reviewerID':0.2, ..}}, got type = {type(ans1)}\"\n",
    "assert type(ans2) == dict, f\"answer to question 2 must be a dictionary like {{'asin':0.2, ..}}, got type = {type(ans2)}\"\n",
    "assert type(ans3) == float, f\"answer to question 3 must be a float like 0.8, got type = {type(ans3)}\"\n",
    "assert type(ans4) == dict, f\"answer to question 4 must be a dictionary like {{'mean':0.4,'max':0.6,'median':0.6...}}, got type = {type(ans4)}\"\n",
    "assert type(ans5) == dict, f\"answer to question 5 must be a dictionary, got type = {type(ans5)}\"         \n",
    "assert ans6 == 0 or ans6==1, f\"answer to question 6 must be 0 or 1, got value = {ans6}\" \n",
    "assert ans7 == 0 or ans7==1, f\"answer to question 7 must be 0 or 1, got value = {ans7}\" \n",
    "\n",
    "ans_dict = {\n",
    "    \"q1\": ans1,\n",
    "    \"q2\": ans2,\n",
    "    \"q3\": ans3,\n",
    "    \"q4\": ans4,\n",
    "    \"q5\": ans5,\n",
    "    \"q6\": ans6,\n",
    "    \"q7\": ans7,\n",
    "    \"runtime\": end-start\n",
    "}\n",
    "with open('my_results_PA1.json', 'w') as outfile: json.dump(ans_dict, outfile)         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
